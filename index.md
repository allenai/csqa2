## **Exposing the limits of AI through Gamification**

MultiModalQA is a challenging question answering dataset that requires joint reasoning over text, tables and images, consisting of [29,918](https://github.com/allenai/MultiModalQA)
examples. 



This dataset was created by a team of [NLP researchers](#authors) at [Tel Aviv University](https://www.tau-nlp.org/), [University of Washington](https://www.cs.washington.edu/research/nlp) and [Allen Institute for AI](https://allenai.org/).

For more details check out our ICLR21
paper ["MultiModalQA: Complex Question Answering over Text, Tables and Images"](https://openreview.net/pdf?id=ee6W5UgQLa),

<center>
    <a href="https://allenai.github.io/MultiModalQA/figures/intro.png"> 
        <img src="figures/intro.png" height="350">
      </a>
</center>

<center>
  <a rel=“license” href=“http://creativecommons.org/licenses/by/4.0/”>
    <img alt=“Creative Commons License” style=“border-width:0" src=“https://i.creativecommons.org/l/by/4.0/88x31.png” />
  </a><br/>
  This work is licensed under a 
  <a rel=“license” href=“http://creativecommons.org/licenses/by/4.0/”>Creative Commons Attribution 4.0 International License</a>
</center>
